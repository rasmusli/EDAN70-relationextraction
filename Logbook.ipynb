{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Logbook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rasmusli/EDAN70-relationextraction/blob/master/Logbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1-RurnpOF5-",
        "colab_type": "text"
      },
      "source": [
        "# Rasmus and Viktor Logbook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCb8iiqdOF6D",
        "colab_type": "text"
      },
      "source": [
        "## 2020 - 04 - 11\n",
        "**Aim**: Setup up notebook, and github. Read articles relating to relation extraction and BioBERT. The main goal now is to get a good grip on what the overall goal of the project is as well as obtain as much information related to the areas as possible.\n",
        "\n",
        "**Result**: Notebook and git now up. Learnt a lot about NLP in general, and relation extraction and BioBERT specifically. Though a lot of more reading to be done.\n",
        "\n",
        "**Next Step**: Keep on reading up on relation extraction and BioBERT. Start setting everything up on our computers.\n",
        "\n",
        "Articles read:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FbYX66OF6D",
        "colab_type": "text"
      },
      "source": [
        "- https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200699 - Relationship extraction using EnsambleSVM \n",
        "- https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2604-0 - Performance study, diff. teqn.\n",
        "- https://www.youtube.com/watch?v=nC9_RfjYwqA - Stanford depency parsing lecture\n",
        "- https://academic.oup.com/bioinformatics/article/36/4/1234/5566506 - BioBERT\n",
        "\n",
        "Also read previous work on relation ship extraction done by:\n",
        "- Vilhelm Lundqvist and Olof Nordengren\n",
        "- Hannes Berntsson\n",
        "\n",
        "Previous group who have worked a bit with BioBERT:\n",
        "- Emil Aminy and Petter Berntsson\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4AoDH2nOF6E",
        "colab_type": "text"
      },
      "source": [
        "## 2020 - 04 - 13\n",
        "**Aim**: Download kaggle dataset, comm_use_subset_100 dataset and set up BioBERT environment.\n",
        "\n",
        "**Result**: Downloaded the datasets. Have started working on Colab and understand how it works. Still working on setting up BioBERT environment.\n",
        "\n",
        "**Next Step**: Set up BioBERT enviroment in Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7keVgtbOF6F",
        "colab_type": "text"
      },
      "source": [
        "Have downloaded BioBERT but as Anton suggests it might take too long time not running it on Colab. https://github.com/AnttonLA/BINP37/blob/master/bioBERT.ipynb . Therefore we will try setting up Colab-environment straight away.\n",
        "\n",
        "CUDA 9.0 download instructions:\n",
        "https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1604&target_type=deblocal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTPpWvAyOF6F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We had some struggles setting up BioBERT. We had to re-think how our final enviroment should look like. The biggest issue lied in that Colab requires us to set up the enviroment every time, so we wanted to make this process as smooth as possible. \n",
        "\n",
        "We will have Colab, google drive and a git as our main enviroment. On git we will have all required files needed for our Colab setup. In drive we will have our Colab notebooks, and from there we can save them to git so that we can show our progress continuously.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwPLSYFaOamq",
        "colab_type": "text"
      },
      "source": [
        "## 2020 - 04 - 27\n",
        "**Aim**: Setup working BioBERT enviroment\n",
        "\n",
        "**Result**: We have run BioBert on the RE and NER train datasets but we have some trouble converting the comm_use_subset_100 JSON data format to the tsv format that BioBERT requires.\n",
        "\n",
        "**Next Step**: Figure out how to convert the data to the required format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddywAWwiOlrT",
        "colab_type": "text"
      },
      "source": [
        "Once again, we have struggled a bit with setting up the BioBERT enviroment. We wanted to avoid needing to authorize to google drive every time we wanted to run the environment. As mentioned in previous entry the plan was to have all the files needed on git. Though we have realized that due to the size of some of the files, for example the pretrained weights, this is not possible. Therefore we have gone back to the solution to store most of the stuff on a google drive and load everthing from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1cYkLqA6G5f",
        "colab_type": "text"
      },
      "source": [
        "BioBERT github link: https://github.com/dmis-lab/biobert?fbclid=IwAR24HALSNntfFeZS3vWSjVRSVTJqsy56wuJS-KOZIwUNn0loVPVQ-waYTLM. We followed the instructions for RE and will now start looking at NER.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnnX_2-jTfg",
        "colab_type": "text"
      },
      "source": [
        "Running the large pretrained weights we got an OOM-error. So instead we resorted to the base pretrained weights. We managed to run the base pretrained BioBERT weights on relationship extraction. For the first GAD data set (downloaded from: https://drive.google.com/file/d/1-jDKGcXREb2X9xTFnuiJ36PvsqoyHWcw/view), with three epochs, this was the results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHSgmBQVdZ_N",
        "colab_type": "text"
      },
      "source": [
        "**GAD Corpora**\n",
        "\n",
        "The corpus contains annotations on the relashionship between genes and deseases and has been developed from the Genetic Association Database (GAD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rp7I5p8jrvp",
        "colab_type": "text"
      },
      "source": [
        "f1 score    : 83.42%\n",
        "\n",
        "recall      : 90.39%\n",
        "\n",
        "precision   : 77.44%\n",
        "\n",
        "specificity : 70.75%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ng57Ebm5zQ",
        "colab_type": "text"
      },
      "source": [
        "For GAD/2 with 3 epochs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZezxU57m9Q8",
        "colab_type": "text"
      },
      "source": [
        "f1 score    : 83.01%\n",
        "\n",
        "recall      : 90.71%\n",
        "\n",
        "precision   : 76.51%\n",
        "\n",
        "specificity : 69.17%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGW3lJaox4mE",
        "colab_type": "text"
      },
      "source": [
        "For GAD/2 with 10 epochs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld9AlyHNyB52",
        "colab_type": "text"
      },
      "source": [
        "f1 score    : 81.55%\n",
        "\n",
        "recall      : 82.86%\n",
        "\n",
        "precision   : 80.28%\n",
        "\n",
        "specificity : 77.47%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puupPhbqP05L",
        "colab_type": "text"
      },
      "source": [
        "Resource links:\n",
        "\n",
        "https://medium.com/@seannaren/cord-19-ann-semantic-search-engine-using-s-bert-aebc5bcc5442 - CORD-19 using BioBERT\n",
        "\n",
        "https://drive.google.com/file/d/1-jDKGcXREb2X9xTFnuiJ36PvsqoyHWcw/view - Relation extraction data (GAD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v7SZ5g5fDoE",
        "colab_type": "text"
      },
      "source": [
        "## 2020 - 05 - 04\n",
        "**Aim**: Run BioBERT NER on the comm_use_subset And create Pubannotation output\n",
        "\n",
        "**Result**: We managed to produce the results in pubannotations but looking at the files we noticed that the denotations \"id\" and \"obj\" were different to the gold standard examples in the BioNLP repository, we need to talk to Sonja about this\n",
        "\n",
        "**Next Step**: Talk to Sonja and discuss our results and about the next step in the project\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lXkHv8Rfiwu",
        "colab_type": "text"
      },
      "source": [
        "Antton gave us his python script for generating txt-files from the json format so the first step today is to generate all the right formats for BioBERT. https://github.com/AnttonLA/BINP37/blob/master/dataset_generation/json_to_txt.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85X5VJoyjjO9",
        "colab_type": "text"
      },
      "source": [
        "When we have the txt-files we can run the BioBERT NER to generate the NER evaluation that we will have to restructure into Pubannotation. https://github.com/AnttonLA/BINP37/blob/master/output_generation/eval_to_pubannot.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC-HGe55ZI8E",
        "colab_type": "text"
      },
      "source": [
        "First the model was trained on the BC5CDR-Chem data downloaded from the NER train data found on link: https://drive.google.com/file/d/1OletxmPYNkz2ltOr9pyT0b0iBtUWxslh/view. The trained model was then used to find named entity on the comm_use_subset_100 in txt format that we created using Antton's convertion script json_to_txt.py.\n",
        "Finally the evaluation was detokenized and transformed to pubannotations using Anttons's eval_to_pubanot.py script found in the link above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y-XWPN0T9xJ",
        "colab_type": "text"
      },
      "source": [
        "# 2020 - 05 - 07\n",
        "**Aim**: We now need to create a tsv file with CLS tokens for the subset to do relationship extraction on it.\n",
        "\n",
        "**Result**: Notebook and git now up. Learnt a lot about NLP in general, and relation extraction and BioBERT specifically. Though a lot of more reading to be done.\n",
        "\n",
        "**Next Step**: Keep on reading up on relation extraction and BioBERT. Start setting everything up on our computers.\n",
        "\n",
        "Articles read:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O2mhgSiUjWN",
        "colab_type": "text"
      },
      "source": [
        "We had a bit of struggle understanding how to be able to convert the files to TSV. With some discussion with Sonja we have concluded that Anton will provide us with the NER-files for gene and disease as a starting point. We will then combine these and split them into sentences. We will discard any sentences not having both of the entities and then create a TSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyNPVDGnbe9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}